{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Snowpark functions\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.functions import lit\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "# Encoding, training, prediction\n",
    "import xgboost as xgb\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "conn = Session.builder.configs(connection_parameters).create()\n",
    "conn.sql_simplifier_enabled = True\n",
    "snowflake_environment = conn.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below step is not required if you have all the data in a single schema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection_GDC_churn.json'))\n",
    "conn_churn = Session.builder.configs(connection_parameters).create()\n",
    "conn_churn.sql_simplifier_enabled = True\n",
    "snowflake_environment = conn_churn.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting customer churn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import customer and subscription data from snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = conn.table('SPOTFLIX.PUBLIC.\"dim_media_customers\"').toPandas()\n",
    "print(customers.head(2))\n",
    "\n",
    "subscriptions = conn.table('SPOTFLIX.PUBLIC.\"fact_media_subscription_events\"').toPandas()\n",
    "print (subscriptions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(customers, subscriptions, how ='left', on = 'customer_id')\n",
    "\n",
    "# Below code checks the following conditions : closedate = eventdate , event = Cancelled Subscription\n",
    "# merged[(merged['event'] == 'Cancelled Subscription') & (merged['closedate'] != merged['eventdate'])]\n",
    "\n",
    "# Dealing with bad data - details below\n",
    "# there are few cancelled subscription dates not mentioned as close date, so correcting them (updating closedate= eventdate when event= Cancelled Subscription\t)\n",
    "merged['closedate'] = np.where(merged['event'] == 'Cancelled Subscription', merged['eventdate'], merged['closedate'])\n",
    "\n",
    "# dropping duplicates\n",
    "merged.drop_duplicates(subset='customer_id', inplace=True)\n",
    "print (merged.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id    opendate   closedate\n",
      "0            1  2018-03-09        None\n",
      "1            2  2017-10-18        None\n",
      "2            3  2018-03-03  2019-04-23\n",
      "3            4  2018-03-25  2018-07-30\n",
      "4            5  2017-07-17  2019-04-24\n"
     ]
    }
   ],
   "source": [
    "customers_selected = merged[['customer_id', 'opendate', 'closedate']]\n",
    "print( customers_selected.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'opendate' and 'closedate' to datetime format\n",
    "customers_selected['opendate'] = pd.to_datetime(customers_selected['opendate'])\n",
    "customers_selected['closedate'] = pd.to_datetime(customers_selected['closedate'])\n",
    "\n",
    "# Calculate the 'days' column without using lambda\n",
    "customers_selected['No_days_on_platform'] = (customers_selected['closedate'] - customers_selected['opendate']).dt.days.fillna((datetime(2019, 7, 26) - customers_selected['opendate']).dt.days)\n",
    "\n",
    "# Create the 'churn' column\n",
    "customers_selected['churn'] = customers_selected['closedate'].notna().astype(int)\n",
    "\n",
    "customers_selected.drop(['opendate', 'closedate'],axis =1, inplace =True )\n",
    "\n",
    "# Display the modified DataFrame\n",
    "customers_selected.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Fact media show event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>viewdate</th>\n",
       "      <th>viewruntime</th>\n",
       "      <th>No_days_on_platform</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23927</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>24</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23927</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>24</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id   viewdate  viewruntime  No_days_on_platform  churn\n",
       "0        23927 2018-07-22           24                108.0      1\n",
       "1        23927 2018-07-22           24                108.0      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_events = conn.table('SPOTFLIX.PUBLIC.\"fact_media_show_events\"').toPandas()\n",
    "show_events.drop(['record_id','show_id', 'viewtime','episode_id','viewruntimepct'], axis=1, inplace=True)\n",
    "# Convert 'viewdate' to datetime\n",
    "show_events['viewdate'] = pd.to_datetime(show_events['viewdate'])\n",
    "customer_engagement = pd.merge(show_events,customers_selected, how = 'left', on = 'customer_id')\n",
    "customer_engagement.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Date Reference Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>viewdate</th>\n",
       "      <th>viewruntime</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23927</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23927</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id   viewdate  viewruntime  churn\n",
       "0        23927 2018-07-22           24      1\n",
       "1        23927 2018-07-22           24      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_ref = conn.table('SPOTFLIX.PUBLIC.\"dim_media_date_ref\"').toPandas()\n",
    "date_ref.drop('date_id', axis = 1, inplace = True) \n",
    "date_ref['date'] = pd.to_datetime(date_ref['date'])\n",
    "customer_engagement['viewdate'] = pd.to_datetime(customer_engagement['viewdate'])\n",
    "date_wise_merged = pd.merge(date_ref, customer_engagement, left_on='date', right_on='viewdate', how='right')\n",
    "date_wise_merged.drop(['date','No_days_on_platform'], axis = 1, inplace =True\t)\n",
    "date_wise_merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>viewdate</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>viewruntime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>577</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>579</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    viewdate  customer_id  viewruntime\n",
       "0 2018-06-17          577           58\n",
       "1 2018-06-17          579           58"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_wise_merged['viewdate'] = pd.to_datetime(date_wise_merged['viewdate'])\n",
    "\n",
    "grouped_date_wise = date_wise_merged.groupby(['viewdate', 'customer_id']).agg({\n",
    "    'viewruntime': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "grouped_date_wise.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that generates information required for model training\n",
    "#This function uses date array from inout data to build necessary columns for model training\n",
    "def date_conv(date_array):\n",
    "    date_array_df = pd.DataFrame(date_array.values, columns = ['viewdate'])\n",
    "    date_array_df['viewdate'] = pd.to_datetime(date_array_df['viewdate'])\n",
    "    date_array_df['day_of_year'] = date_array_df['viewdate'].dt.dayofyear\n",
    "    date_array_df['day_of_month'] = date_array_df['viewdate'].dt.day\n",
    "    date_array_df['day_of_week'] = date_array_df['viewdate'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    date_array_df['month'] = date_array_df['viewdate'].dt.month\n",
    "    date_array_df['quarter'] = date_array_df['viewdate'].dt.quarter\n",
    "    date_array_df['year'] = date_array_df['viewdate'].dt.year\n",
    "    return date_array_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates training dataset at specified granularity - eg: customer , show, date level\n",
    "def make_dataset(df, date_col, customer_id_col, agg_col, agg_func):\n",
    "    grp_df = df.groupby([date_col, customer_id_col]).agg({\n",
    "        agg_col: agg_func,\n",
    "    }).reset_index()\n",
    "    pivot_df = grp_df.pivot(index=date_col, columns=customer_id_col, values=agg_col)\n",
    "\n",
    "    itr = len(pivot_df.columns)\n",
    "    df_list = []\n",
    "    keys = pivot_df.columns.to_list()\n",
    "\n",
    "    for i in range(itr):\n",
    "        col = pivot_df.columns[i]\n",
    "        df1 = pd.DataFrame(pivot_df[col])\n",
    "\n",
    "        # Drop NaN values using the aggregated column name\n",
    "        df1.dropna(subset=[col], inplace=True)\n",
    "\n",
    "        training_data = date_conv(df1.index)\n",
    "        training_data = training_data.set_index(date_col)\n",
    "        training_data[agg_col] = df1[col]\n",
    "        df_list.append(training_data)\n",
    "\n",
    "    return df_list, keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset_test, keys = make_dataset(date_wise_merged, 'viewdate', 'customer_id', 'viewruntime', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series forecasting model using xgboost\n",
    "# Forecasts data for multiple shows, customers, dates etc.,\n",
    "def train_xgboost_models(category_data_list, keys,agg_col,folder_path):\n",
    "    fi = []\n",
    "    fn = []\n",
    "    for i in range(len(category_data_list)):\n",
    "        # for category, data in category_data.items():\n",
    "            # Convert the date index\n",
    "\n",
    "        print (category_data_list[i])\n",
    "\n",
    "        training_data = date_conv(category_data_list[i].index)\n",
    "        print(training_data)\n",
    "\n",
    "        # Set the index to 'Transaction_Date'\n",
    "        # training_data = training_data.set_index('Transaction_Date')\n",
    "        feature_cols = category_data_list[i].columns[:-1]\n",
    "        print(feature_cols)\n",
    "\n",
    "        # Set the 'Sales_Quantity' column to the corresponding category\n",
    "        # training_data['Sales_Quantity'] = keys\n",
    "\n",
    "        # Select features for training the model\n",
    "        # feature_cols = ['day_of_year', 'day_of_month', 'day_of_week', 'month', 'quarter', 'year']\n",
    "        X = category_data_list[i].drop(agg_col, axis=1)\n",
    "        y_quantity = category_data_list[i][agg_col]\n",
    "\n",
    "        # Define the best hyperparameters obtained from hyperparameter tuning\n",
    "        best_params = {\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.3,\n",
    "            'gamma': 0,\n",
    "            'subsample': 1\n",
    "        }\n",
    "\n",
    "        # Initialize XGBoost model with the best hyperparameters\n",
    "        model = xgb.XGBRegressor(n_estimators=200, n_jobs=1,\n",
    "                                    max_depth=best_params['max_depth'],\n",
    "                                    eta=best_params['eta'],\n",
    "                                    gamma=best_params['gamma'],\n",
    "                                    subsample=best_params['subsample']\n",
    "                                    )\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X, y_quantity)\n",
    "\n",
    "\n",
    "        # folder_path = '/Users/mohammed.arshad/Downloads/Models'\n",
    "        filename = f'{folder_path}/xgb_classifier_{keys[i]}.sav'\n",
    "        joblib.dump(model, filename)\n",
    "\n",
    "\n",
    "        # Feature importance\n",
    "        feat_importance = pd.DataFrame(\n",
    "            model.feature_importances_, feature_cols, columns=[\"FeatImportance\"]\n",
    "        ).to_dict()\n",
    "\n",
    "        fi.append(feat_importance)\n",
    "        fn.append(filename)\n",
    "\n",
    "    return fi, fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_col = 'viewruntime'\n",
    "folder_path = 'Models'\n",
    "feature_Imp, models = train_xgboost_models(make_dataset_test[0:100], keys,agg_col, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling the data for faster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trained = date_wise_merged[date_wise_merged['customer_id']<=100]\n",
    "data_trained .drop('churn',inplace = True, axis =1)\n",
    "data_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method used to forecast data using the trained models at customer level\n",
    "def forecast_and_add_to_dataframe(model_folder,data_trained):\n",
    "    # Check if the model folder exists\n",
    "    if not os.path.exists(model_folder):\n",
    "        raise FileNotFoundError(f\"The model folder '{model_folder}' does not exist.\")\n",
    "    \n",
    "    forecast_end_date = '2021-01-31'\n",
    "    predicted_df = pd.DataFrame()\n",
    "    final_df= pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Loop through each file in the model folder\n",
    "    for filename in os.listdir(model_folder):\n",
    "        # Assuming models are saved with a specific extension, e.g., '.pkl'\n",
    "        if filename.endswith('.sav'):\n",
    "            # Construct the full path to the model file\n",
    "            model_path = os.path.join(model_folder, filename)\n",
    "            # print(model_path)\n",
    "            # Load the model\n",
    "            model = joblib.load(model_path)  # Adjust this for other model loading methods\n",
    "\n",
    "\n",
    "\n",
    "# one line to get customer_id from filename  - os.path.splitext(filename.split(\"_\")[-1])[0]\n",
    "# slice the data_trained using customer id  and sort by view date .iloc[-1,'viewdate']\n",
    "           \n",
    "            customer_id = int(os.path.splitext(filename.split(\"_\")[-1])[0])\n",
    "            print(customer_id)\n",
    "\n",
    "            # Slice data_trained and sort by 'viewdate' for the specific customer_id\n",
    "            last_available_date = data_trained[data_trained['customer_id'] == customer_id].sort_values('viewdate').iloc[-1]['viewdate']\n",
    "            # print (last_available_date)\n",
    "\n",
    "            # Display the last available date for the specified customer_id\n",
    "            # print(f\"Last available date for customer {customer_id}: {last_available_date}\")\n",
    "\n",
    "\n",
    "            forecast_start_date = last_available_date + pd.DateOffset(days=1)\n",
    "            # print (forecast_start_date)\n",
    "            forecast_start_date=pd.to_datetime(forecast_start_date)\n",
    "            forecast_end_date=pd.to_datetime(forecast_end_date)\n",
    "            date_array = pd.date_range(forecast_start_date,forecast_end_date, freq='D')\n",
    "            forecast_data = date_conv(date_array).set_index('viewdate')\n",
    "            # print (forecast_data)\n",
    "\n",
    "            # Make predictions on the new data\n",
    "            predictions = model.predict(forecast_data)\n",
    "\n",
    "            # print (predictions)\n",
    "\n",
    "            print(len(forecast_data))\n",
    "            print(len(predictions))\n",
    "\n",
    "\n",
    "            # Create a DataFrame for the predicted data\n",
    "            predicted_df = pd.DataFrame({\n",
    "                'customer_id': [int(os.path.splitext(filename.split(\"_\")[-1])[0])] * len(predictions),\n",
    "                'viewruntime': predictions,\n",
    "                'viewdate': forecast_data.index,  # Assuming 'forecast_data' has 'viewdate' as an index\n",
    "            })\n",
    "\n",
    "            # Concatenate the predicted data to the final_df\n",
    "            frames = [final_df, predicted_df]\n",
    "            final_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        # Sort the final_df by 'viewdate' if needed\n",
    "        final_df.sort_values('viewdate', inplace=True)\n",
    "\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using created models to run forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "model_folder_path = 'Models'\n",
    "\n",
    "# Assuming forecast_data is your DataFrame with the data for forecasting\n",
    "forecast_data = forecast_and_add_to_dataframe(model_folder_path,data_trained)\n",
    "forecast_data.reset_index(drop=True, inplace=True)\n",
    "forecasted_churn = pd.concat([data_trained, forecast_data], ignore_index=True)\n",
    "forecasted_churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing forecasted data to Snowflake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_churn['viewdate'] = forecasted_churn['viewdate'].dt.date\n",
    "conn_churn.createDataFrame(forecasted_churn).write.mode('overwrite').save_as_table('forecasted_viewtime_100_customers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
